{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer exploration\n",
    "Megatron has a bunch of tokenizer options. Let's get a sense for what they do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import data_utils\n",
    "from data_utils import tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<data_utils.lazy_loader.lazy_array_loader object at 0x1a2b61f3c8>\n"
     ]
    }
   ],
   "source": [
    "# Make a dataset. These args were extracted using pdb.\n",
    "kwargs = {'path_': '/Users/ben/data/tiny.json',\n",
    " 'binarize_sent': False, 'delim': ',', 'drop_unlabeled': False, 'label_key': 'label',\n",
    " 'lazy': True, 'loose': True, 'process_fn': None, 'text_key': 'text'}\n",
    "\n",
    "# Below failed with a JSON decode error.\n",
    "# data_utils.get_dataset(kwargs['path_'], **kwargs)\n",
    "text = data_utils.lazy_array_loader(kwargs['path_'], data_type='data', map_fn=None)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading BertWordPieceTokenizer ( bert-large-uncased ) from cache_dir  None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 231508/231508 [00:00<00:00, 847799.93B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded bert-large-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "kwargs = {'tokenizer_type': 'BertWordPieceTokenizer', \n",
    "        'corpus': text, #<data_utils.lazy_loader.lazy_array_loader object at 0x1a29766e80>, \n",
    "        'model_path': 'tokenizer.model', 'vocab_size': 30522, \n",
    "        'model_type': 'bert-large-uncased', 'pad_token': 0, \n",
    "        'character_coverage': 1.0, 'command_tokens': None, 'type_tokens': None, \n",
    "        'kwargs': {\n",
    "            'ds_type': 'BERT', 'cache_dir': 'temp_cache_dir', 'max_preds_per_seq': 80}, \n",
    "        'tokenizer_class': 'BertWordPieceTokenizer'}\n",
    "bert_tokenizer = data_utils.tokenization.BertWordPieceTokenizer(kwargs['model_type'], **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30522 2\n",
      "lorem ipsum ['lore', '##m', 'ip', '##sum'] False\n",
      "lorem ipsum hello! [19544, 2213, 12997, 17421, 7592, 999] True\n"
     ]
    }
   ],
   "source": [
    "print(bert_tokenizer.num_tokens, bert_tokenizer.num_type_tokens)\n",
    "t = bert_tokenizer.EncodeAsTokens('lorem ipsum')\n",
    "print(t.text, t.tokenization, t.asIds)\n",
    "t = bert_tokenizer.EncodeAsIds('lorem ipsum hello!')\n",
    "print(t.text, t.tokenization, t.asIds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from data_utils import bpe_encoder\n",
    "e = bpe_encoder.get_encoder('117M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24494, 16567, 0]\n",
      "50000 256\n",
      "cool beans!\n"
     ]
    }
   ],
   "source": [
    "# test encoding, check vocab size\n",
    "toks = e.encode('cool beans!')\n",
    "print(toks)\n",
    "print(len(e.bpe_ranks), len(e.byte_decoder))\n",
    "print(e.decode(toks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'world']\n",
      "[7592, 2088]\n",
      "<data_utils.tokenization.Tokenization object at 0x1a2d90eda0> [7592, 2088]\n",
      "hello\n"
     ]
    }
   ],
   "source": [
    "# what is bert doing under the hood for EncodeAsIds?\n",
    "from data_utils.tokenization import Tokenization\n",
    "processed_text = 'hello world'\n",
    "tokens = bert_tokenizer.text_tokenizer.tokenize(processed_text)\n",
    "print(tokens)\n",
    "Ids = bert_tokenizer.text_tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(Ids)\n",
    "tok = Tokenization(Ids, processed_text, text)\n",
    "print(tok, tok.tokenization)\n",
    "print(bert_tokenizer.IdToToken(7592))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30522 ['[PAD]', '[unused0]', '[unused1]', '[unused2]', '[unused3]']\n",
      "30522 [('[PAD]', 0), ('[unused0]', 1), ('[unused1]', 2), ('[unused2]', 3), ('[unused3]', 4)]\n",
      "50257 [('!', 0), ('\"', 1), ('#', 2), ('$', 3), ('%', 4), ('&', 5), (\"'\", 6), ('(', 7), (')', 8), ('*', 9)]\n"
     ]
    }
   ],
   "source": [
    "# What's inside these?\n",
    "# self._tokens = list(self.text_tokenizer.vocab.keys())\n",
    "print(len(bert_tokenizer._tokens), bert_tokenizer._tokens[:5])\n",
    "\n",
    "#self._vocab = {k:v for k,v in self.text_tokenizer.vocab.items()}\n",
    "print(len(bert_tokenizer._vocab), list(bert_tokenizer._vocab.items())[:5])\n",
    "\n",
    "# self._text_token_vocab = {k:v for k,v in self.text_tokenizer.vocab.items()}\n",
    "\n",
    "print(len(e.encoder), list(e.encoder.items())[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[31373, 995, 0]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from data_utils.tokenization import Tokenization, CommandToken, TypeToken\n",
    "\n",
    "class BytePairTokenizer(data_utils.tokenization.TextTokenizer):\n",
    "    \"\"\"\n",
    "    Loads a pretrained WordPiece tokenizer from `cache_dir` for tokenization\n",
    "    in BERT training. Default to bert-large-uncased tokenizer.\n",
    "    \"\"\"\n",
    "    def __init__(self, tokenizer_model_type=None, cache_dir=None, encoder=None, **kwargs):\n",
    "        self.encoder = encoder if encoder else bpe_encoder.get_encoder('117M')\n",
    "        # set command tokens from wordpiece tokenizer values\n",
    "        self.num_tokens = len(self.encoder.encoder)\n",
    "        self.num_text_tokens = self.num_tokens\n",
    "\n",
    "        # Probably don't need stuff below\n",
    "        self._command_tokens = [\n",
    "        ]\n",
    "        self.num_command_tokens = len(self._command_tokens)\n",
    "        \n",
    "        self.command_name_map = {tok.name: tok for tok in self._command_tokens}\n",
    "        self.command_token_map = {tok.token: tok for tok in self._command_tokens}\n",
    "        self.command_id_map = {tok.Id: tok for tok in self._command_tokens}\n",
    "\n",
    "        # set type tokens\n",
    "        self.type_tokens = [\n",
    "            TypeToken('str0', '<str0>', 0),\n",
    "            TypeToken('str1', '<str1>', 1),\n",
    "        ]\n",
    "        self.num_type_tokens = len(self.type_tokens)\n",
    "        \n",
    "        self.type_name_map = {tok.name: tok for tok in self.type_tokens}\n",
    "        self.type_token_map = {tok.token: tok for tok in self.type_tokens}\n",
    "        self.type_id_map = {tok.Id: tok for tok in self.type_tokens}\n",
    "\n",
    "        # parse tokens and vocabs from tokenizer\n",
    "\n",
    "        self._tokens = list(self.encoder.encoder.keys())\n",
    "        self._vocab = self.encoder.encoder\n",
    "\n",
    "        self._text_tokens = list(self._tokens)\n",
    "        self._text_token_vocab = self._vocab\n",
    "\n",
    "        self._command_token_tokens = list(self.command_token_map.keys())\n",
    "        self._command_token_vocab = {t:Id for Id,t in self.command_id_map.items()}\n",
    "\n",
    "        self._token_types = list(self.type_token_map.keys())\n",
    "        self._token_type_vocab = {t:Id for Id, t in self.type_id_map.items()}\n",
    "\n",
    "    def EncodeAsIds(self, text, process_fn=None):\n",
    "        \"\"\"convert text to wordpiece Ids\"\"\"\n",
    "        processed_text = text\n",
    "        if process_fn is not None:\n",
    "            processed_text = process_fn(processed_text)\n",
    "        #tokens = self.text_tokenizer.tokenize(processed_text)\n",
    "        Ids = self.encoder.encode(processed_text)\n",
    "        return Tokenization(Ids, processed_text, text)\n",
    "\n",
    "    def EncodeAsTokens(self, text, process_fn=None):\n",
    "        \"\"\"convert wordpiece token to Id\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def IdToToken(self, Id, type_token=False):\n",
    "        \"\"\"convert Id to sentencpiece token\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def TokenToId(self, token, type_token=False):\n",
    "        \"\"\"convert sentencpiece token to Id\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def DecodeIds(self, Ids, type_token=False):\n",
    "        \"\"\"converts ids to wordpiece tokens and joins them as a text string\"\"\"\n",
    "        if isinstance(Ids, Tokenization):\n",
    "            Ids = Ids.tokenization\n",
    "        return self.encoder.decode(Ids)\n",
    "\n",
    "    def DecodeTokens(self, Tokens, type_token=False):\n",
    "        if isinstance(Ids, Tokenization):\n",
    "            Ids = Ids.tokenization\n",
    "        return ' '.join(Tokens)\n",
    "    \n",
    "byte_pair_tokenizer = BytePairTokenizer(kwargs['model_type'], encoder=e, **kwargs)\n",
    "byte_pair_tokenizer.EncodeAsIds('hello world!').tokenization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
